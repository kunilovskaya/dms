{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate frequencies of individual items from tmx, produce results separate for each lang\n",
    "\n",
    "and calculate translationally distictive items from comparison with the nfreqs from the reference corpus(arg4)\n",
    "\n",
    "(based on wilkoxon ranksum test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys,codecs,os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import re\n",
    "from xml.dom import minidom\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg1 = '/home/masha/birmingham/data/EM_my_uniq200.tmx' #sys.argv[1]\n",
    "arg2 = '/home/masha/birmingham/searchlists/en_EMs.ls' #sys.argv[2] # separate lists for each lang to avoid looping over lang on top of all else\n",
    "arg3 = '/home/masha/birmingham/searchlists/ru_EMs.ls' #sys.argv[2]\n",
    "arg4 = '/home/masha/birmingham/data/rnc/EM' #reference corpus for wilk ranksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = minidom.parse(arg1)\n",
    "tus = doc.getElementsByTagName(\"tu\") #возвращает список tu (элемента, содержащего tuvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop over all tuvs in tus to create a list of keys (=fn) in the statistics and restore_text dics; these dics have lists of values for each key\n",
    "fns = []\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "        fn = fn.strip()\n",
    "        if fn not in fns:\n",
    "            fns.append(fn)\n",
    "restore_texts = {fn : [] for fn in fns}\n",
    "statistics = {fn :{} for fn in fns}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762\n"
     ]
    }
   ],
   "source": [
    "# build the restore_texts dic and collect all values (=text segments)\n",
    "print len(tus)\n",
    "fn_in_restore = 0\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "\n",
    "        fn = fn.strip()\n",
    "        seg_el = tuv.getElementsByTagName(\"seg\")[0]\n",
    "\n",
    "        seg_text = seg_el.childNodes[-1].data\n",
    "        seg_text = seg_text.strip()\n",
    "        \n",
    "        restore_texts[fn].append(seg_text) # there are 19524 segs in my_uniq200.tmx\n",
    "    #print len(restore_texts[fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce number of sentences for each file, which can be accessed as restore_segs[fn]\n",
    "\n",
    "for fn, segs in restore_texts.items():\n",
    "    num_SENTs = 0\n",
    "    for seg in segs: #this replace obviously did not work\n",
    "        SENTs = seg.count('_SENT_')\n",
    "        num_SENTs += SENTs\n",
    "    #print fn, '\\t', num_SENTs\n",
    "    statistics[fn]['SENTs'] = num_SENTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count freqs of tagged items - CONNs and EMs, \n",
    "\n",
    "calculate their normalized freqs using stats from statistics and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of search items:  EN:  80 RU:  80\n"
     ]
    }
   ],
   "source": [
    "en_queries = set([item.strip() for item in codecs.open(arg2, 'r', 'utf-8').readlines()])\n",
    "ru_queries = set([item.strip() for item in codecs.open(arg3, 'r', 'utf-8').readlines()])\n",
    "print 'Number of search items: ', 'EN: ', len(en_queries), 'RU: ',len(ru_queries),'\\n'\n",
    "\n",
    "en_total_hits = {item : [] for item in en_queries}\n",
    "en_nfreqs = {item : [] for item in en_queries}\n",
    "for item in en_queries:\n",
    "#     count_item = 0 # counter for freq of all items in this fn\n",
    "    for fn, segs in restore_texts.items(): # segs is a list\n",
    "        string = \" \".join(seg for seg in segs) #convert list of segs to a string to avoid yet another loop\n",
    "        hits = string.count(item) # why did I have .find() here?!! \n",
    "        en_total_hits[item].append(hits)\n",
    "        en_nfreqs[item].append(hits/statistics[fn]['SENTs']*100) # list of normalized freq for each text\n",
    "    #print item.encode('utf-8'), sum(en_total_hits[item])\n",
    "\n",
    "ru_total_hits = {item : [] for item in ru_queries}\n",
    "ru_nfreqs = {item : [] for item in ru_queries}\n",
    "for item in ru_queries:\n",
    "#     count_item = 0 # counter for freq of all items in this fn\n",
    "    for fn, segs in restore_texts.items(): # segs is a list\n",
    "        string = \" \".join(seg for seg in segs) #convert list of segs to a string to avoid yet another loop\n",
    "        hits = string.count(item) # why did I have .find() here?!! \n",
    "        ru_total_hits[item].append(hits)\n",
    "        ru_nfreqs[item].append(hits/statistics[fn]['SENTs']*100) # list of normalized freq for each text\n",
    "    #print item.encode('utf-8'), sum(ru_total_hits[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "read and calculate ref data to detect translationally distictive items thru wilk ranksum comparison with reference\n",
    "'''\n",
    "ref_nfreqs = {item :[] for item in ru_queries}#список частот в текстах корпуса2\n",
    "ref_files = [f for f in os.listdir(arg4) if f.endswith('.em')]\n",
    "count = 0\n",
    "\n",
    "for f in ref_files:\n",
    "    text = codecs.open(arg4+'/'+f,'r','utf-8').read()\n",
    "    txt = codecs.open(arg4+'/'+f,'r','utf-8').readlines()\n",
    "    sents = len(txt)\n",
    "    #print sents\n",
    "    count +=1\n",
    "    for item in ru_queries:\n",
    "        ref_nfreq = text.count(item)/len(txt)*100#normalized to no. of sents\n",
    "        ref_nfreqs[item].append(ref_nfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma\t total\t av_nfreq*100\n",
      "\n",
      "EVEM_полагаюнеобходимым \t0.0 \t0.001 \t0.968439293523 \t-\n",
      "OWEM_безусловно \t0.065 \t0.107 \t0.0444424940955 \t-\n",
      "MWEM_должнобыть \t0.0 \t0.0 \t0.984216558938 \t-\n",
      "OWEM_наверняка \t0.068 \t0.065 \t0.294912528286 \t+\n",
      "EVEM_предполагаю \t0.01 \t0.002 \t0.878849969374 \t+\n",
      "MWEM_наврядли \t0.0 \t0.002 \t0.984216558938 \t-\n",
      "MWEM_можетбыть \t0.023 \t0.07 \t0.0720644496573 \t-\n",
      "OWEM_бесспорно \t0.009 \t0.015 \t0.79513332903 \t-\n",
      "OWEM_наверно \t0.014 \t0.124 \t0.0135040617403 \t-\n",
      "EVEM_какожидается \t0.031 \t0.001 \t0.969858833617 \t+\n",
      "EVEM_допускаю \t0.0 \t0.004 \t0.921206155534 \t-\n",
      "EVEM_несчитаю \t0.008 \t0.003 \t0.924269683364 \t+\n",
      "EVEM_какпредполагается \t0.0 \t0.0 \t1.0 \t-\n",
      "OWEM_пожалуй \t0.059 \t0.138 \t0.00738314139664 \t-\n",
      "EVEM_утверждаю \t0.001 \t0.001 \t0.969937699797 \t-\n",
      "MWEM_смоейточкизрения \t0.0 \t0.008 \t0.827731510749 \t-\n",
      "MWEM_повсейвидимости \t0.0 \t0.012 \t0.734438519984 \t-\n",
      "OWEM_очевидно \t0.13 \t0.144 \t0.0560518255073 \t-\n",
      "EVEM_можноутверждать \t0.016 \t0.014 \t0.870509697372 \t+\n",
      "EVEM_представляется \t0.0 \t0.052 \t0.243133201283 \t-\n",
      "MWEM_какожидается \t0.0 \t0.002 \t0.984216558938 \t-\n",
      "ENEM_высказалипредположение \t0.0 \t0.002 \t0.984216558938 \t-\n",
      "EVEM_можнонесомневаться \t0.0 \t0.0 \t1.0 \t-\n",
      "OWEM_вероятно \t0.139 \t0.079 \t0.731422941922 \t+\n",
      "ENEM_естьпредположениечто \t0.0 \t0.0 \t0.984216558938 \t-\n",
      "EVEM_мнеНЕкажетсячто \t0.0 \t0.003 \t0.985834181945 \t-\n",
      "OWEM_разве \t0.064 \t0.083 \t0.11356176992 \t-\n",
      "ENEM_суверенностьюможноV \t0.001 \t0.008 \t0.765341031394 \t-\n",
      "EAEM_убежден \t0.0 \t0.006 \t0.874249403991 \t-\n",
      "MWEM_намойвзгляд \t0.019 \t0.113 \t0.0189205453499 \t-\n",
      "EVEM_можнодумать \t0.0 \t0.0 \t0.984216558938 \t-\n",
      "ENEM_нетуверенности \t0.0 \t0.001 \t0.984216558938 \t-\n",
      "EVEM_кажетсячто \t0.102 \t0.029 \t0.806447745301 \t+\n",
      "EAEM_уверен \t0.022 \t0.018 \t0.823571864974 \t+\n",
      "OWEM_возможно \t0.392 \t0.251 \t0.0334358618484 \t+\n",
      "OWEM_наверное \t0.008 \t0.115 \t0.017636779788 \t-\n",
      "MWEM_едвали \t0.014 \t0.027 \t0.805758522467 \t-\n",
      "EVEM_надополагать \t0.0 \t0.01 \t0.827731510749 \t-\n",
      "MWEM_такли \t0.003 \t0.013 \t0.852593064718 \t-\n",
      "EVEM_несомневаюсь \t0.0 \t0.003 \t0.952674372155 \t-\n",
      "OWEM_неужели \t0.006 \t0.035 \t0.449765699177 \t-\n",
      "EVEM_надодумать \t0.0 \t0.004 \t0.889860837233 \t-\n",
      "MWEM_помоемуубеждению \t0.0 \t0.004 \t0.921206155534 \t-\n",
      "MWEM_самособойразумеется \t0.001 \t0.003 \t0.998342631781 \t-\n",
      "MWEM_помоему \t0.0 \t0.061 \t0.118088089889 \t-\n",
      "OWEM_маловероятный \t0.0 \t0.001 \t0.985834181945 \t-\n",
      "EVEM_мнеНЕкажется \t0.011 \t0.004 \t0.878849969374 \t+\n",
      "OWEM_кажется \t0.041 \t0.043 \t0.634196773925 \t-\n",
      "OWEM_конечно \t0.426 \t0.669 \t5.79309760539e-12 \t-\n",
      "EAEM_неуверен \t0.0 \t0.002 \t0.952674372155 \t-\n",
      "ENEM_изпредположениячто \t0.0 \t0.0 \t0.984216558938 \t-\n",
      "EVEM_можнопредположить \t0.019 \t0.024 \t0.675323236925 \t-\n",
      "EVEM_полагаю \t0.001 \t0.029 \t0.604723716642 \t-\n",
      "EVEM_думаю \t0.07 \t0.083 \t0.157004231592 \t-\n",
      "EVEM_неверю \t0.0 \t0.0 \t0.984216558938 \t-\n",
      "MWEM_помоемумнению \t0.0 \t0.04 \t0.313009210531 \t-\n",
      "ENEM_помоемумнению \t0.0 \t0.001 \t0.984216558938 \t-\n",
      "OWEM_неужто \t0.0 \t0.004 \t0.905515095889 \t-\n",
      "EVEM_верю \t0.0 \t0.007 \t0.843178854852 \t-\n",
      "ENEM_исходимизпредположениячто \t0.0 \t0.0 \t1.0 \t-\n",
      "OWEM_якобы \t0.029 \t0.048 \t0.285172788985 \t-\n",
      "ENEM_нетсомнениячто \t0.0 \t0.004 \t0.889860837233 \t-\n",
      "MWEM_каквидно \t0.0 \t0.005 \t0.843178854852 \t-\n",
      "OWEM_видимо \t0.0 \t0.122 \t0.0020277849377 \t-\n",
      "MWEM_внесомнения \t0.001 \t0.004 \t0.950901862328 \t-\n",
      "EVEM_какпредставляется \t0.0 \t0.009 \t0.751602988613 \t-\n",
      "MWEM_повидимому \t0.0 \t0.002 \t0.968439293523 \t-\n",
      "MWEM_бытьможет \t0.0 \t0.011 \t0.721771516701 \t-\n",
      "EVEM_догадываюсь \t0.0 \t0.001 \t0.984216558938 \t-\n",
      "OWEM_вроде \t0.018 \t0.075 \t0.100041720151 \t-\n",
      "MWEM_безсомнения \t0.036 \t0.009 \t0.989779834791 \t+\n",
      "EAEM_неубежден \t0.0 \t0.0 \t1.0 \t-\n",
      "EVEM_думается \t0.0 \t0.022 \t0.552855278006 \t-\n",
      "OWEM_разумеется \t0.001 \t0.049 \t0.190522664137 \t-\n",
      "MWEM_повсейвероятности \t0.0 \t0.008 \t0.843178854852 \t-\n",
      "ENEM_Xвызываетпредположениечто \t0.0 \t0.001 \t0.984216558938 \t-\n",
      "MWEM_самособой \t0.001 \t0.004 \t0.919517784497 \t-\n",
      "OWEM_небось \t0.0 \t0.001 \t0.936927948748 \t-\n",
      "MWEM_какпредполагается \t0.0 \t0.0 \t0.984216558938 \t-\n",
      "EVEM_считаю \t0.062 \t0.026 \t0.99045062329 \t+\n",
      "MWEM_inmyopinion \t0.0\n",
      "EAEM_benotlikelyto \t0.0\n",
      "EAEM_likely \t0.314\n",
      "ENEM_bestofmyknowledge \t0.0\n",
      "MWEM_beyonddoubt \t0.001\n",
      "OWEM_certainly \t0.148\n",
      "MWEM_nodoubt \t0.004\n",
      "OWEM_decidedly \t0.011\n",
      "EVEM_seemto \t0.117\n",
      "EAEM_bepossible \t0.029\n",
      "MWEM_frommypointofview \t0.0\n",
      "OWEM_evidently \t0.0\n",
      "EAEM_besure \t0.0\n",
      "MWEM_forsure \t0.0\n",
      "EVEM_iassume \t0.001\n",
      "ENEM_havenodoubt \t0.0\n",
      "EVEM_seemthat \t0.0\n",
      "EAEM_benotlikely \t0.0\n",
      "MWEM_tomymind \t0.0\n",
      "EAEM_becertainto \t0.0\n",
      "OWEM_sure \t0.005\n",
      "EVEM_assumingthat \t0.0\n",
      "EAEM_itisclear \t0.026\n",
      "EAEM_benotsure \t0.0\n",
      "EAEM_benotconvinced \t0.0\n",
      "EVEM_ifind \t0.009\n",
      "EAEM_beobvious \t0.0\n",
      "OWEM_arguably \t0.023\n",
      "MWEM_goeswithoutsaying \t0.0\n",
      "EVEM_doubtthat \t0.0\n",
      "ENEM_havefeeling \t0.0\n",
      "EAEM_beconvinced \t0.0\n",
      "ENEM_therefeelingthat \t0.0\n",
      "MWEM_inmyeyes \t0.0\n",
      "OWEM_maybe \t0.093\n",
      "OWEM_obviously \t0.041\n",
      "EAEM_notunlikely \t0.0\n",
      "OWEM_definitely \t0.012\n",
      "EAEM_unlikely \t0.005\n",
      "EVEM_LINKseems \t0.172\n",
      "EVEM_ifindthat \t0.0\n",
      "EVEM_seemtome \t0.025\n",
      "EVEM_ibelievethat \t0.025\n",
      "EVEM_dontbelieve \t0.008\n",
      "MWEM_iwouldsay \t0.0\n",
      "EAEM_beprobable \t0.0\n",
      "EAEM_benotlikelythat \t0.0\n",
      "OWEM_perhaps \t0.254\n",
      "EVEM_iguess \t0.001\n",
      "EVEM_LINKseem \t0.201\n",
      "EVEM_ithink \t0.088\n",
      "EAEM_itisnotclear \t0.03\n",
      "MWEM_asforme \t0.005\n",
      "OWEM_naturally \t0.0\n",
      "EAEM_benotcertainto \t0.0\n",
      "EAEM_becertain \t0.067\n",
      "EVEM_arguethat \t0.0\n",
      "OWEM_presumably \t0.018\n",
      "EAEM_benatural \t0.0\n",
      "ENEM_therenodoubt \t0.009\n",
      "ENEM_myquessis \t0.0\n",
      "EAEM_likelythat \t0.008\n",
      "EVEM_ifeel \t0.0\n",
      "EVEM_ibelieve \t0.046\n",
      "EAEM_likelyto \t0.183\n",
      "OWEM_probably \t0.222\n",
      "MWEM_ofcourse \t0.186\n",
      "MWEM_sureenough \t0.006\n",
      "ENEM_theredoubt \t0.016\n",
      "OWEM_apparently \t0.028\n",
      "MWEM_astome \t0.0\n",
      "MWEM_inmyview \t0.0\n",
      "ENEM_mybelief \t0.0\n",
      "EVEM_appearto \t0.088\n",
      "ENEM_havedoubt \t0.0\n",
      "EVEM_appearthat \t0.024\n",
      "OWEM_possibly \t0.021\n",
      "MWEM_looklike \t0.0\n",
      "EVEM_canbeargued \t0.0\n",
      "EAEM_benotcertain \t0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print contrastive results for tmx\n",
    "'''\n",
    "\n",
    "print 'lemma', '\\t', 'test_total', '\\t', 'test_av_nfreq*100', '\\t', 'ref_total', '\\t','ref_av_nfreq', '\\t', 'wilk_p', '\\t', 'res'\n",
    "for item in ru_queries:\n",
    "    ru_total = sum(ru_total_hits[item])\n",
    "    ru_av_nfreq = round(np.mean(ru_nfreqs[item]),3) # mean normalized frequency of the item over all texts (*2 accounts for bi-lingual char of the format)\n",
    "    \n",
    "    '''\n",
    "    detect translationally distictive items thru wilk ranksum comparison with reference\n",
    "    '''\n",
    "    \n",
    "    res = None\n",
    "    ref_av_nfreq = round(np.mean(ref_nfreqs[item]), 3)   \n",
    "    p = stats.ranksums(ru_nfreqs[item], ref_nfreqs[item])[1] \n",
    "    if ru_av_nfreq > ref_av_nfreq:\n",
    "        res = '+'\n",
    "    \n",
    "        print item.encode('utf-8'), '\\t', ru_total, '\\t', ru_av_nfreq, '\\t', ref_total, '\\t', ref_av_nfreq, '\\t', p,'\\t',res\n",
    "    else:\n",
    "        res = '-'    \n",
    "        print item.encode('utf-8'), '\\t', ru_total, '\\t', ru_av_nfreq, '\\t', ref_total, '\\t', ref_av_nfreq, '\\t', p,'\\t',res\n",
    "\n",
    "\n",
    "for item in en_queries:\n",
    "    en_total = sum(en_total_hits[item])\n",
    "    en_av_nfreq = round(np.mean(en_nfreqs[item]),3) # mean normalized frequency of the item over all texts (*2 accounts for bi-lingual char of the format)\n",
    "    print item.encode('utf-8'), '\\t', en_total, '\\t', en_av_nfreq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save all results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with codecs.open('/home/masha/birmingham/stats/test_EM_items_uniq200_wilk.tsv', 'w') as outfile:\n",
    "#     writer = csv.writer(outfile, delimiter='\\t')\n",
    "#     writer.writerow(['item'] + ['all_hits'] + ['test_av_nfreq']+ ['ref_av_nfreq']+ ['wilk_p']) #table header (does not print res)\n",
    "#     for item in ru_queries:\n",
    "#         writer.writerow([item.encode('utf-8')] + [sum(ru_total_hits[item])] + [round(np.mean(ru_nfreqs[item]),4)]+ [np.mean(ref_nfreq[item])] + [p])\n",
    "#     for item in en_queries:\n",
    "#         writer.writerow([item.encode('utf-8')] + [sum(en_total_hits[item])] + [round(np.mean(en_nfreqs[item]),4)])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
