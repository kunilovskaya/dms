{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this script produces descriptive stats for corpora in plain txt format (as opposed to tmx)\n",
    "\n",
    "from __future__ import division #по умолчанию при делении 2/345 выдаем нецелые числа (хотя для этого есть float(), но почему-то ниже он выдаёт результат truncated to 0.0)\n",
    "import sys,codecs,os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg1 = '/home/masha/birmingham/searchlists/ru_EMs.ls'#sys.argv[1]\n",
    "arg2 = '/home/masha/birmingham/data/rnc/EM'#sys.argv[2]\n",
    "\n",
    "CONN = set([w.strip() for w in codecs.open(arg1,'r','utf-8').readlines()])\n",
    "\n",
    "corp = [f for f in os.listdir(arg2) if f.endswith('.em')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "значением ключа (doc) будет список частот каждого коннектора из списка в этом doc. \n",
    "Длина списка равна кол-ву коннекторов в списке\n",
    "'''\n",
    "wfreq_dict = {f:[] for f in corp}\n",
    "'''\n",
    "тут cписок нормализованных на размер текста частот для каждого doc в корпусе, \n",
    "длина каждого списка = 1, т.е. filename:nfreq\n",
    "'''\n",
    "nfreq_dict = {f:[] for f in corp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in CONN:\n",
    "    for f in corp:\n",
    "        text = codecs.open(arg2+'/'+f,'r','utf-8').read()\n",
    "        wfreq = text.count(word) #absolute number of hits in each text\n",
    "        '''\n",
    "        словарь абсолютных частот каждого слова (dict value is a list of freqs) в поисковом списке \n",
    "        для каждого текста (key) в корпусе = его длина равна колву текстов в корпусе\n",
    "        '''\n",
    "        wfreq_dict[f].append(wfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp_size = 0\n",
    "corp_freq =[]\n",
    "total_hits = 0\n",
    "#print 'TEXT\\t nfreq*100sents\\t hits\\t text-size \\t no.sents\\n'# \\t overtokens*100 \\t allCONN/oversent*100', '\\n'\n",
    "\n",
    "#comment out this block and the last line in the next loop to avoid overwriting the output file\n",
    "# outfile = codecs.open(\"/home/masha/birmingham/stats/EMdoc_rnc-txt.tsv\", 'w', 'utf-8')\n",
    "# writer = csv.writer(outfile, delimiter='\\t')\n",
    "# writer.writerow(['file'] + ['nfreq*100sents'])\n",
    "\n",
    "for f in corp:\n",
    "    txt = codecs.open(arg2+'/'+f,'r','utf-8').readlines()\n",
    "    sents = len(txt)#колво предложений в тексте\n",
    "    #print sents\n",
    "    text = codecs.open(arg2+'/'+f,'r','utf-8').read()\n",
    "    text_length = len(text.split())#колво токенов в тексте\n",
    "    corp_size = corp_size + text_length #объем корпуса\n",
    "    nfreq_tok = float(sum(wfreq_dict[f])/len(text.split()))*100#нормализация на объем текста в токенах, база нормализации 100 слов\n",
    "    nfreq = float(sum(wfreq_dict[f])/len(txt)*100)#колво всех употреблений всех коннекторов из списка в конкретном тексте на колво предложений\n",
    "    corp_freq.append(nfreq)\n",
    "    total = sum(corp_freq)\n",
    "    total_hits += sum(wfreq_dict[f])\n",
    "#     print f,'\\t',  nfreq, '\\t', sum(wfreq_dict[f]),'\\t', text_length, '\\t', sents #,'\\t',nfreq_tok, '\\t', nfreq   total/len(corp-freq)\n",
    "#     writer.writerow([f.encode('utf-8')] + [nfreq])\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size ( /home/masha/birmingham/data/rnc/EM ) \t3141274\n",
      "Search list size:  \t80\n",
      "Num of docs:  \t1562\n",
      "Total hits:  \t4705\n",
      "Mean for normalized freqs:  \t2.94015678609\n",
      "Standard deviation for normalized freqs:  \t3.14934747764\n"
     ]
    }
   ],
   "source": [
    "print 'Corpus size (',arg2,')', '\\t', corp_size\n",
    "print 'Search list size: ', '\\t', len(wfreq_dict[f])#length of the list which is a value to a filename as a key in this dict is No. of words\n",
    "print 'Num of docs: ', '\\t', len(wfreq_dict)#No of entries in the dictionary (wfreq or nfreq) = No. of files\n",
    "#total/len(corp_freq)\n",
    "print 'Total hits: ', '\\t', total_hits \n",
    "print 'Mean for normalized freqs: ','\\t', np.mean(corp_freq)\n",
    "print 'Standard deviation for normalized freqs: ','\\t', np.std(corp_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
