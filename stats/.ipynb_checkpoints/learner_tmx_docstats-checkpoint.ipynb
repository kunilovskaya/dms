{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate learner parallel corpus size and DM freqs in it, based on FILESOURCES, instead of props\n",
    "import re\n",
    "import sys, codecs\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "from __future__ import division #regular division does not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use prof_tmx_stats.py for tmx with professional translation. It has no attribute filesource!\n",
    "arg1 = '/home/masha/birmingham/data/EM_my_uniq200.tmx' #sys.argv[1]# this is a clean and tidy rltc_tmx! it does not work for prof\n",
    "arg2 = '/home/masha/birmingham/searchlists/bi-ling_EMs.ls' #sys.argv[2]#список поисковых запросов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = minidom.parse(arg1)\n",
    "errors = 0\n",
    "restore_texts = {}\n",
    "\n",
    "tus = doc.getElementsByTagName(\"tu\") #возвращает список tu (элемента, содержащего tuvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop over all tuvs in tus to create a list of keys (=fn) in the statistics and restore_text dics; this dic has a list of values for each key\n",
    "fns = []\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "        if len(fn) == 0:\n",
    "            errors += 1\n",
    "        fn = fn.strip()\n",
    "        if fn not in fns:\n",
    "            fns.append(fn)\n",
    "statistics = {fn :{} for fn in fns}\n",
    "# for key in statistics.keys():\n",
    "#     print key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# build the dic and collect all values (=text segments)\n",
    "print len(tus)\n",
    "\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "        if len(fn) == 0:\n",
    "            errors += 1\n",
    "        fn = fn.strip()\n",
    "        seg_el = tuv.getElementsByTagName(\"seg\")[0]\n",
    "\n",
    "        seg_text = seg_el.childNodes[-1].data\n",
    "        seg_text = seg_text.strip()\n",
    "        \n",
    "        try:\n",
    "            restore_texts[fn].append(seg_text)\n",
    "        except KeyError:\n",
    "            restore_texts[fn] = [seg_text]\n",
    "print errors       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing and printing a dic is a trick\n",
    "\n",
    "for fn, segs in restore_texts.items(): # or .iteritems()\n",
    "    #print fn, '\\t', len(segs)\n",
    "    try:\n",
    "        statistics[fn]['tuvs'].append(len(segs))\n",
    "    except KeyError:\n",
    "        statistics[fn]['tuvs'] = len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce word count and number of sentences for each file, which can be accessed as restore_segs[fn]\n",
    "\n",
    "for fn, segs in restore_texts.items():\n",
    "    num_SENTs = 0\n",
    "    wc = 0\n",
    "    for seg in segs: #this replace obviously did not work\n",
    "        SENTs = seg.count('_SENT_')\n",
    "        words = len(seg.split())\n",
    "        num_SENTs += SENTs\n",
    "        wc += words\n",
    "\n",
    "    #print fn, '\\t', wc, '\\t', num_SENTs\n",
    "    statistics[fn]['SENTs'] = num_SENTs\n",
    "    statistics[fn]['wc'] = wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of search items:  160\n"
     ]
    }
   ],
   "source": [
    "# count freqs of tagged items - CONNs and EMs\n",
    "    \n",
    "queries = set([item.strip() for item in codecs.open(arg2, 'r', 'utf-8').readlines()])\n",
    "print 'Number of search items: ', len(queries)\n",
    "for fn, segs in restore_texts.items():\n",
    "    #print '\\n', fn, '\\n' \n",
    "    count_item = 0\n",
    "    for seg in segs: # other scritpts concatenate all segs into one string to avoid extra looping\n",
    "        for item in queries:\n",
    "            item=item.strip()\n",
    "            hits = seg.count(item) # .find() counts only one occurence of an item in each string, which is most often the case, but..\n",
    "            count_item += hits\n",
    "            \n",
    "            #print count_item\n",
    "            \n",
    "            #print item.encode('utf-8'), count_item\n",
    "\n",
    "    #print fn, '\\t', count_item \n",
    "    statistics[fn]['freq'] = count_item\n",
    "    statistics[fn]['nfreq'] = statistics[fn]['freq']/statistics[fn]['SENTs']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num of sents in EN_sources:  10283\n",
      "Total hits of list items in EN_sources:  467\n",
      "Mean for normalized freqs:  \t5.1441097995\n",
      "Standard deviation for normalized freqs:  \t6.046159401\n",
      "\n",
      "Num of sents in RU_translations:  10346\n",
      "Total hits of list items in RU_translations:  414\n",
      "Mean for normalized freqs:  \t3.83793693338\n",
      "Standard deviation for normalized freqs:  \t5.14708090789\n"
     ]
    }
   ],
   "source": [
    "# print results from the statistics nested dict of {fn : {'tuv' : 2, 'SENTs' : 2, 'wc' : 37, 'freq' : 4, 'nfreg': 0.95}}\n",
    "EN_all_SENTS = 0\n",
    "RU_all_SENTS = 0\n",
    "EN_allhits = 0\n",
    "RU_allhits = 0\n",
    "EN_allnormed_lst = []\n",
    "RU_allnormed_lst = []\n",
    "EN_allnormed = 0\n",
    "RU_allnormed = 0\n",
    "\n",
    "#print 'file', '\\t', 'tuvs', '\\t', \"SENTs\", '\\t', \"wc\", '\\t', 'freq'\n",
    "\n",
    "for fn, nest in statistics.iteritems():\n",
    "    #print fn, '\\t', nest['tuvs'], '\\t', nest['SENTs'], '\\t', nest['wc'], '\\t', nest['freq']\n",
    "    if fn.startswith(\"EN\"):\n",
    "        EN_all_SENTS += nest['SENTs']\n",
    "        EN_allhits += nest['freq']\n",
    "        EN_allnormed += (nest['freq']/nest['SENTs']*100)\n",
    "        EN_allnormed_lst.append(nest['freq']/nest['SENTs']*100)\n",
    "    else: #if fn.startswith(\"RU\"):\n",
    "        RU_all_SENTS += nest['SENTs']\n",
    "        RU_allhits += nest['freq']\n",
    "        RU_allnormed += (nest['freq']/nest['SENTs']*100)\n",
    "        RU_allnormed_lst.append(nest['freq']/nest['SENTs']*100)\n",
    "\n",
    "print '\\n', 'Num of sents in EN_sources: ', EN_all_SENTS \n",
    "print 'Total hits of list items in EN_sources: ', EN_allhits\n",
    "print 'Mean for normalized freqs: ','\\t', np.mean(EN_allnormed_lst)\n",
    "print 'Standard deviation for normalized freqs: ','\\t', np.std(EN_allnormed_lst)\n",
    "\n",
    "print '\\n', 'Num of sents in RU_translations: ', RU_all_SENTS \n",
    "print 'Total hits of list items in RU_translations: ', RU_allhits\n",
    "print 'Mean for normalized freqs: ','\\t', np.mean(RU_allnormed_lst)\n",
    "print 'Standard deviation for normalized freqs: ','\\t', np.std(RU_allnormed_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#запишем-ка в файл\n",
    "with open(\"/home/masha/birmingham/stats/CONN_my_uniq200.tsv\", 'w') as outfile: #adjust outfile names appropriately\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "    writer.writerow(['file'] + ['tuvs'] + ['SENTs'] + ['wc'] + ['freq'] + ['nfreq'])\n",
    "    for fn, nest in statistics.iteritems():\n",
    "        writer.writerow([fn.encode('utf-8')] + [nest['tuvs']] + [nest['SENTs']] + [nest['wc']] + [nest['freq']]+ [nest['nfreq']])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
