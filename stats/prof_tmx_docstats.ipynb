{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "этот скрипт - для стандартных TMX с профессиональными переводами (для учебных переводов другой скрипт, определяющий файл-источник по данным в атрибуте filesource, поскольку prop в rusltc ненадежны из-за множественности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys, codecs, csv\n",
    "import re\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg1 = '/home/masha/birmingham/data/CONN_profmedia.tmx' #sys.argv[1]\n",
    "arg2 = '/home/masha/birmingham/searchlists/bi-ling_CONNs.ls' #sys.argv[2]#список поисковых запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = codecs.open(arg2, 'r', 'utf-8').readlines()\n",
    "doc = minidom.parse(arg1)\n",
    "alltuvs = {}\n",
    "tus = doc.getElementsByTagName(\"tu\") #возвращает список tu (элемента, содержащего анализируемые prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def en_list_fntuvs(doc): #возвращает дедуплицированный список имен файлов из проп\n",
    "    en_fnlst = []\n",
    "    errors = 0\n",
    "    for tu in tus:\n",
    "        props = tu.getElementsByTagName(\"prop\")\n",
    "        #нет prop\n",
    "        if len(props)==0:\n",
    "            errors +=1\n",
    "            continue\n",
    "        prop0 = props[0]\n",
    "        #print prop0\n",
    "        fn = prop0.childNodes[-1].data #fnpair in prop\n",
    "        #print fn, errors\n",
    "        try:\n",
    "            fn = fn.split('-')\n",
    "        except: \n",
    "            continue\n",
    "        en_fn = fn[0]\n",
    "        #print en_fn\n",
    "\n",
    "        en_fnlst.append(en_fn)\n",
    "\n",
    "    en_fnlst = set(en_fnlst) #deduplicate\n",
    "    #print len(en_fnlst)\n",
    "    return en_fnlst # a list of 200 names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ru_list_fntuvs(doc): #то же самое для русского языка\n",
    "    ru_fnlst = []\n",
    "    errors = 0\n",
    "    for tu in tus:\n",
    "        props = tu.getElementsByTagName(\"prop\") # props should have .txt extensions or files _10 and _100 merge     \n",
    "        if len(props)==0:\n",
    "            errors +=1\n",
    "            continue\n",
    "        prop0 = props[0]\n",
    "        #print prop0\n",
    "        fn = prop0.childNodes[-1].data #fnpair in prop\n",
    "        #print fn, errors\n",
    "        try:\n",
    "            fn = fn.split('-')\n",
    "        except: \n",
    "            continue\n",
    "        ru_fn = fn[1]\n",
    "        #print ru_fn\n",
    "\n",
    "        ru_fnlst.append(ru_fn)\n",
    "\n",
    "    ru_fnlst = set(ru_fnlst) #deduplicate\n",
    "    #print len(ru_fnlst)\n",
    "    return ru_fnlst # a list of 200 names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pulldoms_by_lang(lst, lang, alltuvs):\n",
    "    for tu in tus:\n",
    "        props = tu.getElementsByTagName(\"prop\")\n",
    "        if len(props)==0:\n",
    "            continue\n",
    "        prop0 = props[0]\n",
    "        fnpair = prop0.childNodes[-1].data\n",
    "        fnpair = fnpair.strip()\n",
    "        for fn_short in lst:\n",
    "            if fn_short in fnpair:\n",
    "                #print fn\n",
    "                tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "                for tuv in tuvs:\n",
    "                    languag = tuv.getAttributeNode('xml:lang').nodeValue\n",
    "                    if languag == lang:\n",
    "                        #print lang\n",
    "                        alltuvs[fn_short].append(tuv)\n",
    "                    else: \n",
    "                        continue\n",
    "    #print len(alltuvs[fn_short])\n",
    "    return alltuvs #словарь с именами файлов (ключи) и списком tuv (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text(tuv):\n",
    "    segs = tuv.getElementsByTagName(\"seg\")\n",
    "\n",
    "    if len(segs)==0:\n",
    "        return None\n",
    "    seg0 = segs[0]\n",
    "    if len(seg0.childNodes) == 0:\n",
    "        return None \n",
    "    text = seg0.childNodes[-1].data\n",
    "    #print text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dom2txt(fn, dic): \n",
    "    fntuvs = dic[fn] # this is a list of dom for each fn\n",
    "    texts = {fn:[]}\n",
    "    fn_totSENTs = 0\n",
    "    tokens = 0\n",
    "    for fntuv in fntuvs:\n",
    "        fntuv = extract_text(fntuv)#this is content of each seg from tuvs on the list for each fn\n",
    "        #print fntuv.encode('utf-8')\n",
    "        #print type(fntuv)\n",
    "        if fntuv == None:\n",
    "            #print fn\n",
    "            continue\n",
    "        fn_totSENTs += fntuv.count('_SENT_')\n",
    "        tokens += len(fntuv.split())\n",
    "        texts[fn].append(fntuv)\n",
    "    return texts, fn_totSENTs, tokens # this contains a dic which has fn as keys and lists of texted_sents as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sents(fn, dic): #see ru_pickled_sents.py \n",
    "    countSENTs = 0\n",
    "    count_under4 = 0\n",
    "    allsents = dic[fn]# this is a list of all sentences attributed to one fn\n",
    "    for text in allsents:\n",
    "        #print text\n",
    "        if text == None:\n",
    "            count_under4 +=1\n",
    "            continue\n",
    "        if len(text.split())<= 3:\n",
    "            count_under4 +=1\n",
    "            continue\n",
    "        SENTs= text.count('_SENT_') #it should return exactly 1 because text is a list of SENTENCES already after split\n",
    "        countSENTs = countSENTs + SENTs\t\n",
    "        #print countSENTs\n",
    "    return old_countSENTs, count_under4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_each_item(fn, texted):\n",
    "    count_item = 0\n",
    "    allsents = texted[fn]\n",
    "    for text in allsents:\n",
    "        #print text,'\\n'\n",
    "        #text = text.split()\n",
    "        for item in queries:\n",
    "            item=item.strip()\n",
    "            #print item\n",
    "            hits = text.count(item)\n",
    "            count_item += hits\n",
    "    return count_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running my functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = en_list_fntuvs(doc)\n",
    "en_alltuvs = {fn_short.strip(): [] for fn_short in ens}\n",
    "en_alldoms = pulldoms_by_lang(ens, \"EN\", en_alltuvs)\n",
    "count_totenitems = 0\n",
    "count_totruitems = 0\n",
    "en_size = 0\n",
    "en_totsents = 0\n",
    "tot_en_nfreq = []\n",
    "\n",
    "#comment this block and the last lines in the next two loops out to avoid overwriting the output file\n",
    "outfile = codecs.open(\"/home/masha/birmingham/stats/CONNdoc_profmedia.tsv\", 'w', 'utf-8')\n",
    "writer = csv.writer(outfile, delimiter='\\t')\n",
    "writer.writerow(['file'] + ['nfreq*100sents'])\n",
    "\n",
    "#print 'filename', '\\t', 'tuvs', '\\t', 'SENTs', '\\t', 'wc',  '\\t', 'ITEMs', '\\t', 'normed_freq'\n",
    "for en in ens:\n",
    "    en_alltuvs_texted, en_sents, en_tokens = dom2txt(en, en_alldoms)\n",
    "    #print type(en_alltuvs_texted)\n",
    "    #old_en_sents, under4 = count_sents(en, en_alltuvs_texted)\n",
    "    en_items = extract_each_item(en, en_alltuvs_texted)\n",
    "    count_totenitems = count_totenitems + en_items\n",
    "    en_size += en_tokens\n",
    "    en_totsents += en_sents\n",
    "    en_nfreq = en_items/en_sents*100\n",
    "    tot_en_nfreq.append(en_nfreq)\n",
    "    \n",
    "    #print en, '\\t', len(en_alldoms[en]), '\\t', en_sents, '\\t',en_tokens,  '\\t',en_items, '\\t', en_nfreq #, '\\t',under4\n",
    "    writer.writerow([en.encode('utf-8')] + [en_nfreq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rus = ru_list_fntuvs(doc)\n",
    "ru_alltuvs = {fn_short.strip(): [] for fn_short in rus}\n",
    "ru_alldoms = pulldoms_by_lang(rus, \"RU\", ru_alltuvs)\n",
    "ru_size = 0\n",
    "ru_totsents = 0\n",
    "tot_ru_nfreq = []\n",
    "#print 'filename', '\\t', 'tuvs', '\\t', 'SENTs', '\\t', 'wc',  '\\t', 'ITEMs', '\\t', 'normed_freq'\n",
    "for ru in rus:\n",
    "    ru_alltuvs_texted, ru_sents, ru_tokens = dom2txt(ru, ru_alldoms)\n",
    "    #old_ru_sents, under4 = count_sents(ru, ru_alltuvs_texted)\n",
    "    ru_items = extract_each_item(ru, ru_alltuvs_texted)\n",
    "    count_totruitems = count_totruitems + ru_items\n",
    "    ru_size += ru_tokens\n",
    "    ru_totsents += ru_sents\n",
    "    ru_nfreq = ru_items/ru_sents*100\n",
    "    tot_ru_nfreq.append(ru_nfreq)\n",
    "    \n",
    "    #print ru, '\\t', len(ru_alldoms[ru]), '\\t', ru_sents, '\\t', ru_tokens,  '\\t', ru_items, '\\t', ru_nfreq # under4\n",
    "    writer.writerow([ru.encode('utf-8')] + [ru_nfreq])\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSources:  \tprofTargets: \n",
      "Corpus size ( CONN_profmedia.tmx ) \t336976 \t311782\n",
      "Number of all hits:  \t1618 \t2231\n",
      "No. of sents:  \t14324 \t14825\n",
      "Mean for normalized freqs:  \t11.4465586756 \t15.6702601162\n",
      "Standard deviation for normalized freqs:  \t7.02913912142 \t7.89233520793\n"
     ]
    }
   ],
   "source": [
    "print '\\t', 'Sources: ', '\\t', 'profTargets: '\n",
    "print 'Corpus size (', arg1.split('/')[-1], ')', '\\t', en_size, '\\t', ru_size\n",
    "print 'Number of all hits: ', '\\t', count_totenitems, '\\t', count_totruitems\n",
    "print 'No. of sents: ', '\\t', en_totsents, '\\t', ru_totsents\n",
    "print 'Mean for normalized freqs: ', '\\t', np.mean(tot_en_nfreq), '\\t', np.mean(tot_ru_nfreq)\n",
    "print 'Standard deviation for normalized freqs: ', '\\t', np.std(tot_en_nfreq), '\\t', np.std(tot_ru_nfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
