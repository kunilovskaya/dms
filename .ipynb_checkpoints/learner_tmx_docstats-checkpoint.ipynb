{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate learner parallel corpus size and DM freqs in it, based on FILESOURCES, instead of props\n",
    "import re\n",
    "import sys, codecs\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "from __future__ import division #regular division does not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use prof_tmx_stats.py for tmx with professional translation. It has no attribute filesource!\n",
    "arg1 = '/home/masha/birmingham/data/EM_my_uniq200.tmx' #sys.argv[1]# this is a clean and tidy rltc_tmx! it does not work for prof\n",
    "arg2 = '/home/masha/birmingham/searchlists/bi-ling_EMs.ls' #sys.argv[2]#список поисковых запросов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = minidom.parse(arg1)\n",
    "errors = 0\n",
    "restore_texts = {}\n",
    "\n",
    "tus = doc.getElementsByTagName(\"tu\") #возвращает список tu (элемента, содержащего tuvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all tuvs in tus to create a list of keys (=fn) in the statistics and restore_text dics; this dic has a list of values for each key\n",
    "fns = []\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "        if len(fn) == 0:\n",
    "            errors += 1\n",
    "        fn = fn.strip()\n",
    "        if fn not in fns:\n",
    "            fns.append(fn)\n",
    "statistics = {fn :{} for fn in fns}\n",
    "# for key in statistics.keys():\n",
    "#     print key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# build the dic and collect all values (=text segments)\n",
    "print len(tus)\n",
    "\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "        if len(fn) == 0:\n",
    "            errors += 1\n",
    "        fn = fn.strip()\n",
    "        seg_el = tuv.getElementsByTagName(\"seg\")[0]\n",
    "\n",
    "        seg_text = seg_el.childNodes[-1].data\n",
    "        seg_text = seg_text.strip()\n",
    "        \n",
    "        try:\n",
    "            restore_texts[fn].append(seg_text)\n",
    "        except KeyError:\n",
    "            restore_texts[fn] = [seg_text]\n",
    "print errors       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing and printing a dic is a trick\n",
    "\n",
    "for fn, segs in restore_texts.items(): # or .iteritems()\n",
    "    #print fn, '\\t', len(segs)\n",
    "    try:\n",
    "        statistics[fn]['tuvs'].append(len(segs))\n",
    "    except KeyError:\n",
    "        statistics[fn]['tuvs'] = len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce word count and number of sentences for each file, which can be accessed as restore_segs[fn]\n",
    "\n",
    "for fn, segs in restore_texts.items():\n",
    "    num_SENTs = 0\n",
    "    wc = 0\n",
    "    for seg in segs: #this replace obviously did not work\n",
    "        SENTs = seg.count('_SENT_')\n",
    "        words = len(seg.split())\n",
    "        num_SENTs += SENTs\n",
    "        wc += words\n",
    "\n",
    "    #print fn, '\\t', wc, '\\t', num_SENTs\n",
    "    statistics[fn]['SENTs'] = num_SENTs\n",
    "    statistics[fn]['wc'] = wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appending other statistics to nested dics (each with a list of values) as values\n",
    "# for fn, nest in statistics.iteritems():\n",
    "#     print fn, '\\t', nest['tuvs'], '\\t', nest['SENTs'], '\\t', nest['wc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of search items:  160\n",
      "EN_1_263.head.txt \t1\n",
      "RU_1_67_1.head.txt \t20\n",
      "EN_1_48.head.txt \t1\n",
      "EN_1_145.head.txt \t0\n",
      "EN_1_50.head.txt \t1\n",
      "EN_1_296.head.txt \t0\n",
      "EN_4_40.head.txt \t38\n",
      "EN_1_266.head.txt \t1\n",
      "RU_1_289_1.head.txt \t3\n",
      "RU_1_116_1.head.txt \t0\n",
      "EN_1_19.head.txt \t1\n",
      "RU_1_243_1.head.txt \t6\n",
      "RU_1_180_1.head.txt \t3\n",
      "RU_1_139_1.head.txt \t10\n",
      "RU_1_292_1.head.txt \t0\n",
      "EN_1_144.head.txt \t0\n",
      "RU_1_247_1.head.txt \t4\n",
      "EN_3_2.head.txt \t0\n",
      "RU_1_207_1.head.txt \t1\n",
      "EN_1_126.head.txt \t1\n",
      "EN_1_3.head.txt \t1\n",
      "EN_1_52.head.txt \t1\n",
      "EN_1_100.head.txt \t1\n",
      "RU_3_4_1.head.txt \t1\n",
      "RU_1_92_1.head.txt \t2\n",
      "RU_1_170_1.head.txt \t3\n",
      "EN_1_312.head.txt \t1\n",
      "RU_1_80_1.head.txt \t0\n",
      "EN_5_6.head.txt \t0\n",
      "RU_4_39_1.head.txt \t2\n",
      "RU_3_5_1.head.txt \t0\n",
      "RU_6_2_1.head.txt \t0\n",
      "RU_1_203_1.head.txt \t6\n",
      "RU_1_303_1.head.txt \t0\n",
      "EN_1_2.head.txt \t3\n",
      "RU_1_245_1.head.txt \t3\n",
      "EN_1_212.head.txt \t0\n",
      "RU_1_264_1.head.txt \t1\n",
      "RU_4_40_1.head.txt \t26\n",
      "RU_1_237_1.head.txt \t3\n",
      "EN_1_315.head.txt \t1\n",
      "RU_3_13_1.head.txt \t0\n",
      "EN_1_122.head.txt \t3\n",
      "RU_1_68_1.head.txt \t2\n",
      "RU_1_53_1.head.txt \t0\n",
      "RU_1_196_1.head.txt \t0\n",
      "EN_4_38.head.txt \t6\n",
      "EN_3_10.head.txt \t0\n",
      "RU_3_9_1.head.txt \t1\n",
      "EN_1_132.head.txt \t1\n",
      "EN_1_292.head.txt \t0\n",
      "EN_4_44.head.txt \t4\n",
      "EN_1_57.head.txt \t0\n",
      "RU_1_178_1.head.txt \t1\n",
      "EN_1_147.head.txt \t7\n",
      "RU_3_8_1.head.txt \t1\n",
      "RU_1_228_1.head.txt \t0\n",
      "RU_1_23_1.head.txt \t6\n",
      "RU_1_40_1.head.txt \t0\n",
      "EN_1_179.head.txt \t1\n",
      "EN_5_5.head.txt \t3\n",
      "RU_1_309_1.head.txt \t0\n",
      "RU_1_149_1.head.txt \t0\n",
      "EN_1_82.head.txt \t2\n",
      "EN_1_244.head.txt \t0\n",
      "EN_1_133.head.txt \t0\n",
      "EN_1_116.head.txt \t1\n",
      "EN_1_208.head.txt \t0\n",
      "RU_6_10_1.head.txt \t6\n",
      "EN_1_203.head.txt \t2\n",
      "RU_3_12_1.head.txt \t0\n",
      "EN_1_184.head.txt \t4\n",
      "RU_1_146_1.head.txt \t2\n",
      "RU_1_270_1.head.txt \t0\n",
      "RU_1_89_1.head.txt \t0\n",
      "RU_1_286_1.head.txt \t0\n",
      "EN_1_274.head.txt \t0\n",
      "EN_1_206.head.txt \t2\n",
      "RU_1_193_1.head.txt \t0\n",
      "RU_1_56_1.head.txt \t0\n",
      "EN_1_243.head.txt \t13\n",
      "EN_4_19.head.txt \t0\n",
      "EN_1_99.head.txt \t15\n",
      "EN_1_260.head.txt \t1\n",
      "EN_3_15.head.txt \t7\n",
      "RU_1_175_1.head.txt \t5\n",
      "RU_3_6_1.head.txt \t1\n",
      "RU_1_26_1.head.txt \t2\n",
      "RU_2_7_1.head.txt \t0\n",
      "RU_1_130_1.head.txt \t1\n",
      "RU_1_306_1.head.txt \t0\n",
      "EN_1_235.head.txt \t2\n",
      "EN_1_139.head.txt \t10\n",
      "EN_1_207.head.txt \t0\n",
      "EN_1_98.head.txt \t0\n",
      "EN_1_44.head.txt \t2\n",
      "RU_1_123_1.head.txt \t0\n",
      "RU_1_19_1.head.txt \t0\n",
      "RU_1_302_1.head.txt \t3\n",
      "EN_1_283.head.txt \t1\n",
      "EN_1_66.head.txt \t38\n",
      "EN_1_264.head.txt \t2\n",
      "EN_4_36.head.txt \t2\n",
      "EN_1_46.head.txt \t1\n",
      "RU_1_248_1.head.txt \t7\n",
      "RU_1_54_1.head.txt \t0\n",
      "EN_1_317.head.txt \t1\n",
      "RU_1_46_1.head.txt \t2\n",
      "EN_1_83.head.txt \t4\n",
      "EN_1_28.head.txt \t2\n",
      "RU_1_313_1.head.txt \t0\n",
      "EN_1_307.head.txt \t0\n",
      "RU_1_58_1.head.txt \t2\n",
      "RU_1_133_1.head.txt \t0\n",
      "EN_1_134.head.txt \t0\n",
      "RU_1_75_1.head.txt \t0\n",
      "RU_1_295_1.head.txt \t0\n",
      "EN_1_124.head.txt \t1\n",
      "EN_6_2.head.txt \t0\n",
      "RU_3_14_1.head.txt \t0\n",
      "RU_1_114_1.head.txt \t0\n",
      "EN_1_26.head.txt \t2\n",
      "EN_1_79.head.txt \t0\n",
      "EN_1_170.head.txt \t0\n",
      "RU_1_300_1.head.txt \t0\n",
      "EN_4_5.head.txt \t2\n",
      "RU_1_134_1.head.txt \t0\n",
      "EN_4_34.head.txt \t1\n",
      "RU_1_79_1.head.txt \t2\n",
      "RU_4_26_1.head.txt \t1\n",
      "EN_6_22.head.txt \t0\n",
      "EN_1_29.head.txt \t0\n",
      "RU_1_128_1.head.txt \t0\n",
      "EN_1_174.head.txt \t3\n",
      "RU_1_208_1.head.txt \t0\n",
      "EN_1_58.head.txt \t3\n",
      "EN_1_289.head.txt \t3\n",
      "EN_1_25.head.txt \t2\n",
      "RU_4_7_1.head.txt \t1\n",
      "EN_3_6.head.txt \t2\n",
      "EN_1_120.head.txt \t1\n",
      "RU_4_38_1.head.txt \t5\n",
      "EN_1_22.head.txt \t3\n",
      "EN_1_36.head.txt \t1\n",
      "RU_1_143_1.head.txt \t0\n",
      "RU_1_296_1.head.txt \t0\n",
      "RU_4_47_1.head.txt \t1\n",
      "RU_1_90_1.head.txt \t0\n",
      "EN_4_39.head.txt \t2\n",
      "RU_1_42_1.head.txt \t0\n",
      "RU_1_312_1.head.txt \t3\n",
      "EN_3_8.head.txt \t1\n",
      "RU_1_266_1.head.txt \t1\n",
      "RU_1_262_1.head.txt \t0\n",
      "RU_1_265_1.head.txt \t1\n",
      "RU_1_82_1.head.txt \t1\n",
      "RU_1_124_1.head.txt \t1\n",
      "EN_1_290.head.txt \t1\n",
      "RU_1_99_1.head.txt \t15\n",
      "RU_1_69_1.head.txt \t0\n",
      "RU_1_97_1.head.txt \t0\n",
      "EN_1_210.head.txt \t1\n",
      "EN_2_7.head.txt \t0\n",
      "RU_1_5_1.head.txt \t1\n",
      "EN_1_313.head.txt \t0\n",
      "EN_1_59.head.txt \t0\n",
      "EN_1_299.head.txt \t0\n",
      "EN_3_7.head.txt \t0\n",
      "RU_4_44_1.head.txt \t4\n",
      "RU_4_42_1.head.txt \t3\n",
      "EN_1_55.head.txt \t0\n",
      "EN_4_33.head.txt \t5\n",
      "RU_1_44_1.head.txt \t2\n",
      "EN_1_65.head.txt \t0\n",
      "RU_3_15_1.head.txt \t16\n",
      "EN_4_45.head.txt \t0\n",
      "EN_1_293.head.txt \t0\n",
      "EN_4_43.head.txt \t0\n",
      "EN_1_295.head.txt \t2\n",
      "RU_1_294_1.head.txt \t3\n",
      "RU_4_43_1.head.txt \t0\n",
      "RU_1_47_1.head.txt \t0\n",
      "EN_1_85.head.txt \t0\n",
      "RU_1_98_1.head.txt \t0\n",
      "RU_1_83_1.head.txt \t0\n",
      "RU_1_3_1.head.txt \t1\n",
      "RU_1_147_1.head.txt \t18\n",
      "RU_1_49_1.head.txt \t1\n",
      "RU_1_86_1.head.txt \t1\n",
      "EN_1_136.head.txt \t1\n",
      "RU_4_32_1.head.txt \t71\n",
      "RU_1_290_1.head.txt \t0\n",
      "EN_1_47.head.txt \t0\n",
      "EN_4_16.head.txt \t0\n",
      "RU_1_316_1.head.txt \t0\n",
      "EN_1_84.head.txt \t4\n",
      "RU_1_129_1.head.txt \t1\n",
      "EN_1_205.head.txt \t0\n",
      "RU_1_29_1.head.txt \t0\n",
      "RU_4_16_1.head.txt \t1\n",
      "EN_1_6.head.txt \t4\n",
      "EN_3_12.head.txt \t0\n",
      "EN_1_54.head.txt \t0\n",
      "RU_1_132_1.head.txt \t0\n",
      "RU_1_96_1.head.txt \t0\n",
      "RU_1_36_1.head.txt \t1\n",
      "EN_1_68.head.txt \t2\n",
      "RU_1_172_1.head.txt \t1\n",
      "RU_1_210_1.head.txt \t1\n",
      "EN_1_60.head.txt \t1\n",
      "EN_1_262.head.txt \t1\n",
      "EN_1_130.head.txt \t0\n",
      "RU_1_59_1.head.txt \t0\n",
      "RU_1_145_1.head.txt \t0\n",
      "EN_1_182.head.txt \t8\n",
      "EN_1_247.head.txt \t0\n",
      "EN_1_89.head.txt \t0\n",
      "RU_4_5_1.head.txt \t0\n",
      "RU_1_212_1.head.txt \t3\n",
      "RU_1_204_1.head.txt \t1\n",
      "EN_1_123.head.txt \t0\n",
      "EN_1_149.head.txt \t0\n",
      "EN_1_7.head.txt \t1\n",
      "EN_1_169.head.txt \t1\n",
      "EN_5_4.head.txt \t0\n",
      "RU_1_307_1.head.txt \t0\n",
      "EN_1_306.head.txt \t2\n",
      "RU_1_169_1.head.txt \t2\n",
      "RU_1_235_1.head.txt \t2\n",
      "RU_1_100_1.head.txt \t1\n",
      "EN_1_303.head.txt \t0\n",
      "RU_3_10_1.head.txt \t1\n",
      "RU_1_297_1.head.txt \t1\n",
      "EN_1_24.head.txt \t5\n",
      "RU_1_176_1.head.txt \t0\n",
      "EN_1_270.head.txt \t0\n",
      "EN_1_76.head.txt \t3\n",
      "EN_3_13.head.txt \t0\n",
      "RU_1_288_1.head.txt \t0\n",
      "RU_1_127_1.head.txt \t1\n",
      "EN_1_4.head.txt \t4\n",
      "RU_3_2_1.head.txt \t1\n",
      "RU_1_106_1.head.txt \t0\n",
      "EN_1_127.head.txt \t2\n",
      "RU_1_66_1.head.txt \t35\n",
      "EN_4_18.head.txt \t0\n",
      "EN_1_45.head.txt \t0\n",
      "RU_1_233_1.head.txt \t1\n",
      "EN_1_106.head.txt \t0\n",
      "EN_1_102.head.txt \t2\n",
      "EN_1_69.head.txt \t0\n",
      "RU_1_179_1.head.txt \t1\n",
      "RU_1_6_1.head.txt \t3\n",
      "EN_1_288.head.txt \t0\n",
      "EN_6_10.head.txt \t7\n",
      "RU_1_283_1.head.txt \t0\n",
      "RU_1_76_1.head.txt \t0\n",
      "EN_1_56.head.txt \t0\n",
      "RU_1_185_1.head.txt \t2\n",
      "RU_4_45_1.head.txt \t0\n",
      "RU_1_200_1.head.txt \t0\n",
      "RU_1_299_1.head.txt \t1\n",
      "EN_1_175.head.txt \t2\n",
      "RU_4_25_1.head.txt \t2\n",
      "RU_5_5_1.head.txt \t1\n",
      "EN_1_146.head.txt \t1\n",
      "EN_1_228.head.txt \t0\n",
      "EN_1_49.head.txt \t1\n",
      "RU_3_7_1.head.txt \t0\n",
      "RU_1_65_1.head.txt \t0\n",
      "RU_4_19_1.head.txt \t0\n",
      "RU_4_36_1.head.txt \t5\n",
      "RU_1_50_1.head.txt \t2\n",
      "RU_1_102_1.head.txt \t2\n",
      "RU_5_9_1.head.txt \t0\n",
      "EN_1_178.head.txt \t3\n",
      "RU_1_52_1.head.txt \t2\n",
      "EN_3_14.head.txt \t2\n",
      "RU_1_206_1.head.txt \t0\n",
      "RU_1_84_1.head.txt \t0\n",
      "EN_4_17.head.txt \t1\n",
      "EN_1_233.head.txt \t0\n",
      "RU_1_48_1.head.txt \t0\n",
      "RU_1_274_1.head.txt \t0\n",
      "RU_1_205_1.head.txt \t0\n",
      "RU_1_60_1.head.txt \t0\n",
      "RU_1_2_1.head.txt \t2\n",
      "EN_1_237.head.txt \t0\n",
      "RU_1_244_1.head.txt \t1\n",
      "EN_1_305.head.txt \t0\n",
      "RU_4_17_1.head.txt \t2\n",
      "RU_1_298_1.head.txt \t0\n",
      "EN_1_245.head.txt \t2\n",
      "EN_1_92.head.txt \t3\n",
      "EN_1_194.head.txt \t0\n",
      "RU_1_314_1.head.txt \t0\n",
      "EN_1_80.head.txt \t2\n",
      "EN_1_240.head.txt \t4\n",
      "EN_3_9.head.txt \t0\n",
      "EN_1_42.head.txt \t0\n",
      "RU_1_194_1.head.txt \t0\n",
      "RU_1_209_1.head.txt \t1\n",
      "EN_1_5.head.txt \t2\n",
      "EN_1_314.head.txt \t1\n",
      "EN_4_25.head.txt \t2\n",
      "RU_1_148_1.head.txt \t0\n",
      "EN_1_193.head.txt \t0\n",
      "EN_1_297.head.txt \t2\n",
      "EN_1_8.head.txt \t1\n",
      "EN_4_7.head.txt \t0\n",
      "RU_1_24_1.head.txt \t1\n",
      "EN_1_196.head.txt \t1\n",
      "EN_1_265.head.txt \t2\n",
      "EN_1_67.head.txt \t11\n",
      "EN_1_176.head.txt \t3\n",
      "EN_1_298.head.txt \t3\n",
      "EN_1_148.head.txt \t0\n",
      "EN_1_294.head.txt \t5\n",
      "EN_4_42.head.txt \t4\n",
      "EN_3_4.head.txt \t1\n",
      "EN_1_316.head.txt \t3\n",
      "RU_1_88_1.head.txt \t2\n",
      "RU_1_43_1.head.txt \t0\n",
      "EN_1_129.head.txt \t1\n",
      "EN_1_180.head.txt \t5\n",
      "RU_1_122_1.head.txt \t0\n",
      "RU_4_18_1.head.txt \t0\n",
      "RU_1_85_1.head.txt \t0\n",
      "RU_1_305_1.head.txt \t0\n",
      "RU_1_7_1.head.txt \t2\n",
      "RU_1_45_1.head.txt \t0\n",
      "EN_1_137.head.txt \t5\n",
      "RU_1_315_1.head.txt \t1\n",
      "RU_1_55_1.head.txt \t0\n",
      "RU_1_260_1.head.txt \t0\n",
      "RU_4_33_1.head.txt \t3\n",
      "RU_1_273_1.head.txt \t2\n",
      "EN_1_114.head.txt \t2\n",
      "RU_1_263_1.head.txt \t0\n",
      "EN_6_3.head.txt \t1\n",
      "EN_5_9.head.txt \t0\n",
      "EN_1_200.head.txt \t0\n",
      "EN_1_86.head.txt \t0\n",
      "EN_1_248.head.txt \t6\n",
      "EN_4_35.head.txt \t2\n",
      "RU_4_37_1.head.txt \t5\n",
      "EN_1_75.head.txt \t0\n",
      "EN_1_273.head.txt \t3\n",
      "RU_1_304_1.head.txt \t0\n",
      "RU_5_6_1.head.txt \t0\n",
      "RU_1_137_1.head.txt \t2\n",
      "EN_3_5.head.txt \t0\n",
      "RU_1_317_1.head.txt \t0\n",
      "EN_4_37.head.txt \t4\n",
      "EN_1_128.head.txt \t0\n",
      "RU_6_3_1.head.txt \t1\n",
      "EN_1_88.head.txt \t3\n",
      "EN_1_246.head.txt \t5\n",
      "EN_1_53.head.txt \t0\n",
      "RU_1_28_1.head.txt \t1\n",
      "EN_1_309.head.txt \t1\n",
      "RU_5_4_1.head.txt \t0\n",
      "EN_1_204.head.txt \t2\n",
      "EN_1_40.head.txt \t2\n",
      "RU_4_34_1.head.txt \t0\n",
      "RU_1_246_1.head.txt \t1\n",
      "EN_1_304.head.txt \t2\n",
      "EN_1_90.head.txt \t0\n",
      "RU_1_8_1.head.txt \t0\n",
      "EN_1_172.head.txt \t0\n",
      "EN_1_286.head.txt \t0\n",
      "EN_1_242.head.txt \t0\n",
      "EN_1_23.head.txt \t7\n",
      "EN_4_26.head.txt \t2\n",
      "RU_1_136_1.head.txt \t1\n",
      "RU_1_240_1.head.txt \t1\n",
      "RU_1_126_1.head.txt \t3\n",
      "EN_1_96.head.txt \t3\n",
      "EN_4_32.head.txt \t61\n",
      "RU_4_35_1.head.txt \t1\n",
      "EN_4_47.head.txt \t2\n",
      "RU_1_242_1.head.txt \t0\n",
      "EN_1_300.head.txt \t0\n",
      "EN_1_143.head.txt \t1\n",
      "RU_6_22_1.head.txt \t0\n",
      "RU_1_182_1.head.txt \t3\n",
      "EN_1_97.head.txt \t0\n",
      "EN_1_185.head.txt \t3\n",
      "EN_1_209.head.txt \t1\n",
      "RU_1_4_1.head.txt \t2\n",
      "RU_1_144_1.head.txt \t1\n",
      "RU_1_120_1.head.txt \t0\n",
      "EN_1_43.head.txt \t0\n",
      "RU_1_22_1.head.txt \t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN_1_302.head.txt \t1\n",
      "RU_1_25_1.head.txt \t4\n",
      "RU_1_184_1.head.txt \t0\n",
      "RU_1_57_1.head.txt \t0\n",
      "RU_1_293_1.head.txt \t2\n",
      "RU_1_174_1.head.txt \t3\n"
     ]
    }
   ],
   "source": [
    "# count freqs of tagged items - CONNs and EMs\n",
    "    \n",
    "queries = set([item.strip() for item in codecs.open(arg2, 'r', 'utf-8').readlines()])\n",
    "print 'Number of search items: ', len(queries)\n",
    "for fn, segs in restore_texts.items():\n",
    "    count_item = 0\n",
    "    for seg in segs:\n",
    "        for item in queries:\n",
    "            item=item.lstrip()\n",
    "            #print item\n",
    "            hits = seg.count(item) # .find() counts only one occurence of an item in each string, which is most often the case, but..\n",
    "            count_item += hits\n",
    " \n",
    "    print fn, '\\t', count_item \n",
    "    statistics[fn]['freq'] = count_item\n",
    "    statistics[fn]['nfreq'] = statistics[fn]['freq']/statistics[fn]['SENTs']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num of sents in EN_sources:  10283\n",
      "Total hits of list items in EN_sources:  460\n",
      "Mean for normalized freqs:  \t5.00945916118\n",
      "Standard deviation for normalized freqs:  \t5.89236518638\n",
      "\n",
      "Num of sents in RU_translations:  10346\n",
      "Total hits of list items in RU_translations:  414\n",
      "Mean for normalized freqs:  \t3.83793693338\n",
      "Standard deviation for normalized freqs:  \t5.14708090789\n"
     ]
    }
   ],
   "source": [
    "# print results from the statistics nested dict of {fn : {'tuv' : 2, 'SENTs' : 2, 'wc' : 37, 'freq' : 4, 'nfreg': 0.95}}\n",
    "EN_all_SENTS = 0\n",
    "RU_all_SENTS = 0\n",
    "EN_allhits = 0\n",
    "RU_allhits = 0\n",
    "EN_allnormed_lst = []\n",
    "RU_allnormed_lst = []\n",
    "EN_allnormed = 0\n",
    "RU_allnormed = 0\n",
    "\n",
    "#print 'file', '\\t', 'tuvs', '\\t', \"SENTs\", '\\t', \"wc\", '\\t', 'freq'\n",
    "\n",
    "for fn, nest in statistics.iteritems():\n",
    "    #print fn, '\\t', nest['tuvs'], '\\t', nest['SENTs'], '\\t', nest['wc'], '\\t', nest['freq']\n",
    "    if fn.startswith(\"EN\"):\n",
    "        EN_all_SENTS += nest['SENTs']\n",
    "        EN_allhits += nest['freq']\n",
    "        EN_allnormed += (nest['freq']/nest['SENTs']*100)\n",
    "        EN_allnormed_lst.append(nest['freq']/nest['SENTs']*100)\n",
    "    else: #if fn.startswith(\"RU\"):\n",
    "        RU_all_SENTS += nest['SENTs']\n",
    "        RU_allhits += nest['freq']\n",
    "        RU_allnormed += (nest['freq']/nest['SENTs']*100)\n",
    "        RU_allnormed_lst.append(nest['freq']/nest['SENTs']*100)\n",
    "\n",
    "print '\\n', 'Num of sents in EN_sources: ', EN_all_SENTS \n",
    "print 'Total hits of list items in EN_sources: ', EN_allhits\n",
    "print 'Mean for normalized freqs: ','\\t', np.mean(EN_allnormed_lst)\n",
    "print 'Standard deviation for normalized freqs: ','\\t', np.std(EN_allnormed_lst)\n",
    "\n",
    "print '\\n', 'Num of sents in RU_translations: ', RU_all_SENTS \n",
    "print 'Total hits of list items in RU_translations: ', RU_allhits\n",
    "print 'Mean for normalized freqs: ','\\t', np.mean(RU_allnormed_lst)\n",
    "print 'Standard deviation for normalized freqs: ','\\t', np.std(RU_allnormed_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запишем-ка в файл\n",
    "# with open(\"/home/masha/birmingham/stats/CONN_my_uniq200.tsv\", 'w') as outfile: #adjust outfile names appropriately\n",
    "#     writer = csv.writer(outfile, delimiter='\\t')\n",
    "#     writer.writerow(['file'] + ['tuvs'] + ['SENTs'] + ['wc'] + ['freq'] + ['nfreq'])\n",
    "#     for fn, nest in statistics.iteritems():\n",
    "#         writer.writerow([fn.encode('utf-8')] + [nest['tuvs']] + [nest['SENTs']] + [nest['wc']] + [nest['freq']]+ [nest['nfreq']])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
