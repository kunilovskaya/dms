{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate learner parallel corpus size and DM freqs in it, based on FILESOURCES, instead of props\n",
    "import re\n",
    "import sys, codecs\n",
    "from xml.dom import minidom\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "from __future__ import division #regular division does not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use prof_tmx_stats.py for tmx with professional translation. It has no attribute filesource!\n",
    "arg1 = '/home/masha/birmingham/data/test_EM_uniq.tmx' #sys.argv[1]# this is a clean and tidy rltc_tmx! it does not work for prof\n",
    "arg2 = '/home/masha/birmingham/searchlists/testbi-ling_EMs.ls' #sys.argv[2]#список поисковых запросов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = minidom.parse(arg1)\n",
    "errors = 0\n",
    "restore_texts = {}\n",
    "\n",
    "tus = doc.getElementsByTagName(\"tu\") #возвращает список tu (элемента, содержащего tuvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all tuvs in tus to create a list of keys (=fn) in the statistics and restore_text dics; this dic has a list of values for each key\n",
    "fns = []\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "        if len(fn) == 0:\n",
    "            errors += 1\n",
    "        fn = fn.strip()\n",
    "        if fn not in fns:\n",
    "            fns.append(fn)\n",
    "statistics = {fn :{} for fn in fns}\n",
    "# for key in statistics.keys():\n",
    "#     print key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# build the dic and collect all values (=text segments)\n",
    "print len(tus)\n",
    "\n",
    "for tu in tus:\n",
    "    tuvs = tu.getElementsByTagName(\"tuv\")\n",
    "    for tuv in tuvs:\n",
    "        fn = tuv.getAttribute('filesource')\n",
    "        if len(fn) == 0:\n",
    "            errors += 1\n",
    "        fn = fn.strip()\n",
    "        seg_el = tuv.getElementsByTagName(\"seg\")[0]\n",
    "\n",
    "        seg_text = seg_el.childNodes[-1].data\n",
    "        seg_text = seg_text.strip()\n",
    "        \n",
    "        try:\n",
    "            restore_texts[fn].append(seg_text)\n",
    "        except KeyError:\n",
    "            restore_texts[fn] = [seg_text]\n",
    "print errors       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# writing and printing a dic is a trick\n",
    "\n",
    "for fn, segs in restore_texts.items(): # or .iteritems()\n",
    "    #print fn, '\\t', len(segs)\n",
    "    try:\n",
    "        statistics[fn]['tuvs'].append(len(segs))\n",
    "    except KeyError:\n",
    "        statistics[fn]['tuvs'] = len(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# produce word count and number of sentences for each file, which can be accessed as restore_segs[fn]\n",
    "\n",
    "for fn, segs in restore_texts.items():\n",
    "    num_SENTs = 0\n",
    "    wc = 0\n",
    "    for seg in segs: #this replace obviously did not work\n",
    "        SENTs = seg.count('_SENT_')\n",
    "        words = len(seg.split())\n",
    "        num_SENTs += SENTs\n",
    "        wc += words\n",
    "\n",
    "    #print fn, '\\t', wc, '\\t', num_SENTs\n",
    "    statistics[fn]['SENTs'] = num_SENTs\n",
    "    statistics[fn]['wc'] = wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of search items:  10\n",
      "4 \n",
      "\n",
      "Hits for all items for each seg in each fn\n",
      "<type 'list'>\n",
      "set([u'EAEM_benotconvinced', u'EAEM_benatural', u'EAEM_beconvinced', u'EAEM_\\u0443\\u0431\\u0435\\u0436\\u0434\\u0435\\u043d', u'\\ufeffEAEM_becertain', u'EAEM_\\u0443\\u0432\\u0435\\u0440\\u0435\\u043d', u'EAEM_benotcertainto', u'EAEM_benotlikely', u'EAEM_becertainto', u'EAEM_benotcertain'])\n",
      "\n",
      "\n",
      "This count has a mistake: EN_6_3.head.txt has 4 hits, not 2!\n",
      "EN_6_3.head.txt \t2\n",
      "<type 'list'>\n",
      "set([u'EAEM_benotconvinced', u'EAEM_benatural', u'EAEM_beconvinced', u'EAEM_\\u0443\\u0431\\u0435\\u0436\\u0434\\u0435\\u043d', u'\\ufeffEAEM_becertain', u'EAEM_\\u0443\\u0432\\u0435\\u0440\\u0435\\u043d', u'EAEM_benotcertainto', u'EAEM_benotlikely', u'EAEM_becertainto', u'EAEM_benotcertain'])\n",
      "\n",
      "\n",
      "This count has a mistake: EN_6_3.head.txt has 4 hits, not 2!\n",
      "RU_6_3_1.head.txt \t2\n",
      "<type 'list'>\n",
      "set([u'EAEM_benotconvinced', u'EAEM_benatural', u'EAEM_beconvinced', u'EAEM_\\u0443\\u0431\\u0435\\u0436\\u0434\\u0435\\u043d', u'\\ufeffEAEM_becertain', u'EAEM_\\u0443\\u0432\\u0435\\u0440\\u0435\\u043d', u'EAEM_benotcertainto', u'EAEM_benotlikely', u'EAEM_becertainto', u'EAEM_benotcertain'])\n",
      "\n",
      "\n",
      "This count has a mistake: EN_6_3.head.txt has 4 hits, not 2!\n",
      "RU_4_37_1.head.txt \t1\n",
      "<type 'list'>\n",
      "set([u'EAEM_benotconvinced', u'EAEM_benatural', u'EAEM_beconvinced', u'EAEM_\\u0443\\u0431\\u0435\\u0436\\u0434\\u0435\\u043d', u'\\ufeffEAEM_becertain', u'EAEM_\\u0443\\u0432\\u0435\\u0440\\u0435\\u043d', u'EAEM_benotcertainto', u'EAEM_benotlikely', u'EAEM_becertainto', u'EAEM_benotcertain'])\n",
      "\n",
      "\n",
      "This count has a mistake: EN_6_3.head.txt has 4 hits, not 2!\n",
      "EN_4_37.head.txt \t0\n"
     ]
    }
   ],
   "source": [
    "# count freqs of tagged items - CONNs and EMs\n",
    "    \n",
    "queries = set([item.strip() for item in codecs.open(arg2, 'r', 'utf-8').readlines()])\n",
    "print 'Number of search items: ', len(queries)\n",
    "print len(restore_texts), '\\n'\n",
    "print 'Hits for all items for each seg in each fn'\n",
    "for fn, segs in restore_texts.items():\n",
    "    #print '\\n', fn, '\\n' \n",
    "    count_item = 0\n",
    "    print type(segs)\n",
    "    print queries\n",
    "    for seg in segs:\n",
    "        for item in queries:\n",
    "            item=item.strip()\n",
    "            '''\n",
    "            Despite \"The method count() returns count of how many times obj occurs in list,\" \n",
    "            count() does not work with segs list!\n",
    "            !!!why does it skip the first list element? (EAEM_becertain) Works if the list starts with a blank line\n",
    "            '''\n",
    "            hits = seg.count(item) # .find() counts only one occurence of an item in each string, which is most often the case, but..\n",
    "            count_item += hits\n",
    "            \n",
    "            #print count_item\n",
    "            \n",
    "            #print item.encode('utf-8'), count_item\n",
    " \n",
    "    print '\\n'\n",
    "    print 'This count has a mistake: EN_6_3.head.txt has 4 hits, not 2!'\n",
    "    print fn, '\\t', count_item \n",
    "    statistics[fn]['freq'] = count_item\n",
    "    statistics[fn]['nfreq'] = statistics[fn]['freq']/statistics[fn]['SENTs']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results from the statistics nested dict of {fn : {'tuv' : 2, 'SENTs' : 2, 'wc' : 37, 'freq' : 4, 'nfreg': 0.95}}\n",
    "EN_all_SENTS = 0\n",
    "RU_all_SENTS = 0\n",
    "EN_allhits = 0\n",
    "RU_allhits = 0\n",
    "EN_allnormed_lst = []\n",
    "RU_allnormed_lst = []\n",
    "EN_allnormed = 0\n",
    "RU_allnormed = 0\n",
    "\n",
    "#print 'file', '\\t', 'tuvs', '\\t', \"SENTs\", '\\t', \"wc\", '\\t', 'freq'\n",
    "\n",
    "for fn, nest in statistics.iteritems():\n",
    "    #print fn, '\\t', nest['tuvs'], '\\t', nest['SENTs'], '\\t', nest['wc'], '\\t', nest['freq']\n",
    "    if fn.startswith(\"EN\"):\n",
    "        EN_all_SENTS += nest['SENTs']\n",
    "        EN_allhits += nest['freq']\n",
    "        EN_allnormed += (nest['freq']/nest['SENTs']*100)\n",
    "        EN_allnormed_lst.append(nest['freq']/nest['SENTs']*100)\n",
    "    else: #if fn.startswith(\"RU\"):\n",
    "        RU_all_SENTS += nest['SENTs']\n",
    "        RU_allhits += nest['freq']\n",
    "        RU_allnormed += (nest['freq']/nest['SENTs']*100)\n",
    "        RU_allnormed_lst.append(nest['freq']/nest['SENTs']*100)\n",
    "\n",
    "print '\\n', 'Num of sents in EN_sources: ', EN_all_SENTS \n",
    "print 'Total hits of list items in EN_sources: ', EN_allhits\n",
    "print 'Mean for normalized freqs: ','\\t', np.mean(EN_allnormed_lst)\n",
    "print 'Standard deviation for normalized freqs: ','\\t', np.std(EN_allnormed_lst)\n",
    "\n",
    "print '\\n', 'Num of sents in RU_translations: ', RU_all_SENTS \n",
    "print 'Total hits of list items in RU_translations: ', RU_allhits\n",
    "print 'Mean for normalized freqs: ','\\t', np.mean(RU_allnormed_lst)\n",
    "print 'Standard deviation for normalized freqs: ','\\t', np.std(RU_allnormed_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запишем-ка в файл\n",
    "# with open(\"/home/masha/birmingham/stats/CONN_my_uniq200.tsv\", 'w') as outfile: #adjust outfile names appropriately\n",
    "#     writer = csv.writer(outfile, delimiter='\\t')\n",
    "#     writer.writerow(['file'] + ['tuvs'] + ['SENTs'] + ['wc'] + ['freq'] + ['nfreq'])\n",
    "#     for fn, nest in statistics.iteritems():\n",
    "#         writer.writerow([fn.encode('utf-8')] + [nest['tuvs']] + [nest['SENTs']] + [nest['wc']] + [nest['freq']]+ [nest['nfreq']])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
